# Классификатор мужских и женских голосов на базе LibriTTS

Данный проект является тестовым заданием для практики в лабораторию Huawei CBG AI.

## Общая структура репозитория

```
project
│   README.md   
│
└───data - данные LibriTTS + примеры голосов, показанные в инференсе
│   │   train-clean-100 - тренировочная выборка (нужно скачать и распаковать архив)
│   │   dev-clean - валидационная выборка (нужно скачать и распаковать архив)
│   │   test-clean - тестовая выборка без шума (нужно скачать и распаковать архив)
│   │   test-other - тестовая выборка с шумом (нужно скачать и распаковать архив)
│   │   male.wav 
│   │   me.wav
│   
└─── logs - логи тренировки моделей
│   │   svm_logs.log - SVM
│   │   vgg_logs.log - VGG16
│    
└─── models - сохраненные обученые модели
│   │   svm.pkl - SVM (необходимо распаковать svm.zip для получения этого файла)
│   │   vgg16.pkl - VGG16
│    
└─── notebooks - jupyter notebooks с EDA и оценков результатов обучения
│   │   EDA.ipynb - небольшой дескриптивный анализ данных
│   │   Results.ipynb - оценка обученных моделей на тестовом датасете, анализ ошибок
│   │   Inference.ipynb - пример инференса моделей для новых данных
│    
└─── src - скрипты
│   │   data.py - функции для препроцессинга данных
│   │   train_svm.py - обучение и подбор гиперпараметров для SVM
│   │   train_cnn.py - обучение нейронной сети с CNN слоями
│   │   inference.py - функции для простого применения моделей для новых звуков
```


Для того, чтобы все команды ниже и код работал, необходимо скачать данные и положить в папку [data](data/),
как это описано в следующем пункте, и разархивировать файл svm.zip в папке [src](src/).
## Краткое описание данных

Как и было предложено в тестовом задании, я взяла данные из базы [LibriTTS](https://research.google/tools/datasets/libri-tts/). 
В базе имеются записи голосов мужчин и женщин, а также расшифровка каждой записи. 
Каждая запись длится нексколько секунд, на ней говорящий произносит одно предложение. 
В базе также имеется информация о соотвествии id говорящего и его/ее пола.

Для обучения и оценки моделей я взяла части датасета, соотвествующие обучающей, валидационной и тестовой выборкам в LibriTTS.
Для того, чтобы запустить код проекта, необходимо скачать архивы **train-clean-100, dev-clean, test-clean, test-other**
с [сайта](https://openslr.org/60/) и распаковать их в папку [data](data/).

Сразу хочется отметить, что изначально в базе присутствует два типа данных - clean (записи голоса без шума) и 
other (вероятно есть шум в записи голоса). Так как для скачивания обучающих данных с шумом (**test-other-500**) 
у меня не хватило бы даже места на ноутбуке на сам архив (как и для части **test-clean-360**, которая содержит 360 часов записей без шума),
было принято решение использовать самый маленький возможный тренировочный датасет **train-clean-100** и для подбора гиперпараметров используется также датасет без шума **dev-clean**.
Однако результаты будут проверяться как на тестовом датасете без шума **test-clean**, так и с шумом **test-other**, 
чтобы оценить, насколько обученные модели хорошо работают на немного другом виде данных.

Еще немного про данные можно посмотреть здесь: [EDA](notebooks/EDA.ipynb).


## Подготовка данных

В качестве препроцессинга данных перед обучением выполняется 
укорачивание/удлинение сигнала до одинаковой длины (в моем случае, 2 секунды), построение мел-спектрограмм 
(как наиболее популярного способа представления звуковых сигналов для алгоритмов). 
Необходимость приведения сигнала к одинаковой длине заключается в том, что для дальнейшего использования признаков, они должны иметь одинаковые размеры.
Более того, на каждой записи представлен лишь один голос, довольно монотонно озвучивающий одно предложение,
поэтому, кажется, что на классификацию голоса диктора на мужской/женский длина записи не оказывает особого влияния, 
поэтому для экономии вычислительных ресурсов и памяти, было решено сократить каждую запись всего до 2 секунд.

Мел-спектрограммы строятся с помощью библиотеки `librosa` с дефолтными параметрами за исключением параметров
fmin и fmax - минимальная и макимальная частота, которые выставлены 20 и 4кГц соответственно. Речь человека 
не выходит за этот частотный диапазон.

Функции для подготовки данных находятся в [файле](src/data.py).

## Эксперименты

В качестве экспериментов решила выбрать одну модель ML в качестве бейзлайна и одну несложную архитектуру нейронной сети.

Выбранная модель для первого этапа - это SVM. В качестве архитектуры нейронной сети выбор пал на VGG, так как работа со звуком, когда
он представлен в виде мел-спектрограмм, схожа с обработкой картинок, а VGG является классической архитектурой для алгоритмов классификации изображений.

Понятно, что для реальной задачи необходимо бы было провести ряд экспериментов с разными моделями и разными архитектурами, 
однако, в данном случае, решила ограничиться лишь этими двумя моделями.

В качестве основной метрики качества была выбрана **accuracy**, именно по значению данной метрики подбираются 
гиперпараметры моделей на валидационном датасете. Сбалансированность датасета по классам (показано в EDA) дает основания полагать,
что данная метрика подходит для нашей задачи. При финальной оценке результатов итоговых моделей будет проведен более подробный анализ с использованием другим метрик классификации.

Далее будут подробнее описаны обучение и оценка SVM и VGG.

### Baseline: SVM

При обучении SVM в тренировочный датасет взято лишь по 10 записей для каждого говорящего для уменьшения выборки до разумного значения, 
которое сможет обработать SVM. Таким образом, всего использовалось 247*10=2470 записей в качестве обучающих примеров. 
Перед применением алгоритма, мел-спетрограмма из двумерного массива растягивалась в одномерный.

На валидационном датасете подбирались параметры kernel и C для SVM по значению accuracy. Скрипт для обучения модели [здесь](src/train_svm.py).

Точная команда для запуска скрипта для обучения SVM и подбора гиперпараметров:

```shell
python3 -m src.train_svm \
    --output-path models/svm.pkl \
    --train-folder data/train-clean-100/LibriTTS/train-clean-100 \
    --dev-folder data/dev-clean/LibriTTS/dev-clean \
    --speaker-data data/train-clean-100/LibriTTS/speakers.tsv > logs/svm_logs.log
```

Лучшая модель по значению accuracy на валидационной выборке оказалась модель с параметрами kernel=rbf и C=1, 
accuracy=0.878.


### VGG

Выбор пал на архитектуру VGG-16, дополнительно к классической реализации добавлены слои дропаута.
Использовался оптимизатор AdamW с learning rate=0.001, в качестве функции потерь использовалась бинарная кросс-энтропия.
Максимальное количество эпох обучения - 50, однако есть условие early stopping - если метрика accuracy не растет на валидациионной
выборке в течение 10 эпох, то обучение останавливается. Лучшая модель по качеству на вал.выборке сохраняется.

Модель обучалась на всех примерах из тренировочного датасета, кол-во примеров: 33236. Для обучения использовалась
одна видеокарточка ноутбука nvidia rtx 3050.

Точная команда для запуска скрипта для обучения VGG:

```shell
python3 -m src.train_cnn \
    --output-path models/vgg16.pkl  \
    --train-folder data/train-clean-100/LibriTTS/train-clean-100 \
    --dev-folder data/dev-clean/LibriTTS/dev-clean \
    --speaker-data data/train-clean-100/LibriTTS/speakers.tsv \
    --batch-size 32 > logs/vgg_logs.log
```

По итогу модель обучалась 17 эпох,
лучший результат на вад.выборке показала на 7 эпохе - accuracy=0.939.


## Результаты

Оценка результатов обеих моделей на тестовых датасетах в ноутбуке [Results](notebooks/Results.ipynb), там же небольшой анализ ошибок.

Результаты на тестовых выборках:

|     | clean | other |
|-----|-------|-------|
| SVM | 0.95  | 0.85  |
| VGG | 0.97  | 0.93  |


## Применение моделей

Функции для применения моделей к новой записи речи находятся в [скрипте](inference.py).
Пример применения обученных моделей можно найти в ноутбуке [Inference](notebooks/Inference.ipynb).

